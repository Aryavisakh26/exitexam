# -*- coding: utf-8 -*-
"""exitexam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JkhusKsc-Hr_1MvF8DGWvIT0X6w-9uVi
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
from sklearn.model_selection import KFold, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
import lightgbm as lgb
import warnings
warnings.filterwarnings("ignore")

# Load Data
# ============================
train = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test_Vges7qu (1).csv')
sample_sub = pd.read_csv('/content/sample_submission_V9Inaty.csv')

TARGET = 'Purchase'

print("Train shape:", train.shape)
print("Test shape:", test.shape)
train.head()

train.info()


print(train.isnull().sum())


train['Purchase'].hist(bins=50)
plt.xlabel("Purchase Amount")
plt.ylabel("Frequency")
plt.title("Purchase Distribution")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.impute import SimpleImputer


target = "Purchase"

cat_cols = ["Gender","Age","City_Category","Stay_In_Current_City_Years","Product_ID","User_ID"]
num_cols = ["Occupation","Marital_Status","Product_Category_1","Product_Category_2","Product_Category_3"]


num_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

cat_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1))
])

preprocessor = ColumnTransformer([
    ("num", num_pipe, num_cols),
    ("cat", cat_pipe, cat_cols)
])

duplicates = train.duplicated().sum()
print("Number of exact duplicate rows in train:", duplicates)

duplicates = test.duplicated().sum()
print("Number of exact duplicate rows in test:", duplicates)

import matplotlib.pyplot as plt
import seaborn as sns

# Copy the train dataset for EDA
eda_df = train.copy()

# 1. Distribution of Target Variable (Purchase Amount)
plt.figure(figsize=(8,5))
sns.histplot(eda_df["Purchase"], bins=40, kde=True)
plt.title("Distribution of Purchase Amount")
plt.show()

# 2. Purchase vs Gender
plt.figure(figsize=(6,4))
sns.barplot(data=eda_df, x="Gender", y="Purchase", estimator="mean", errorbar=None)
plt.title("Average Purchase by Gender")
plt.show()

# 3. Purchase vs Age
plt.figure(figsize=(8,5))
sns.barplot(data=eda_df, x="Age", y="Purchase", estimator="mean", errorbar=None)
plt.title("Average Purchase by Age Group")
plt.show()

# 4. Purchase vs City Category
plt.figure(figsize=(6,4))
sns.barplot(data=eda_df, x="City_Category", y="Purchase", estimator="mean", errorbar=None)
plt.title("Average Purchase by City Category")
plt.show()

# 5. Purchase vs Marital Status
plt.figure(figsize=(6,4))
sns.barplot(data=eda_df, x="Marital_Status", y="Purchase", estimator="mean", errorbar=None)
plt.title("Average Purchase by Marital Status")
plt.show()

# 6. Correlation Heatmap (numerical features)
plt.figure(figsize=(10,6))
sns.heatmap(eda_df.corr(numeric_only=True), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()



#3️⃣ Data Cleaning / Preprocessing
# ============================
def coerce_stay(s):
    return pd.to_numeric(s.astype(str).str.replace(r'\D',''), errors='coerce').fillna(0).astype(int)

for df in [train, test]:
    df['Stay_In_Current_City_Years'] = coerce_stay(df['Stay_In_Current_City_Years'])

for col in ['Product_Category_2','Product_Category_3']:
    train[col] = train[col].fillna(-1).astype(int)
    test[col]  = test[col].fillna(-1).astype(int)

for col in ['User_ID','Product_ID','Gender','Age','City_Category']:
    train[col] = train[col].astype(str)
    test[col]  = test[col].astype(str)

# Frequency encoding for Product_ID
prod_counts = train['Product_ID'].value_counts().to_dict()
train['Product_ID_freq'] = train['Product_ID'].map(prod_counts).fillna(0).astype(int)
test['Product_ID_freq']  = test['Product_ID'].map(prod_counts).fillna(0).astype(int)

#4️⃣ Feature Selection
# ============================
drop_cols = ['User_ID','Product_ID']
X = train.drop(columns=drop_cols + [TARGET], errors='ignore')
y = train[TARGET].copy()
X_test = test.drop(columns=drop_cols, errors='ignore')

# Identify categorical and numerical columns
cat_cols = X.select_dtypes(include='object').columns.tolist()
num_cols = [c for c in X.columns if c not in cat_cols]

# Label encode categorical columns
label_encoders = {}
for col in cat_cols:
    le = LabelEncoder()
    combined = pd.concat([X[col].astype(str), X_test[col].astype(str)], axis=0)
    le.fit(combined)
    X[col] = le.transform(X[col].astype(str))
    X_test[col] = le.transform(X_test[col].astype(str))
    label_encoders[col] = le

# Ensure all features are numeric
X = X.apply(pd.to_numeric, errors='coerce')
X_test = X_test.apply(pd.to_numeric, errors='coerce')

# Fill any remaining NaNs with 0
X = X.fillna(0)
X_test = X_test.fillna(0)



# Fill numeric missing values
X[num_cols] = X[num_cols].fillna(0)
X_test[num_cols] = X_test[num_cols].fillna(0)

#  Optional sampling for quick tests
# ============================
SAMPLE = False
if SAMPLE:
    X_small = X.sample(8000, random_state=42)
    y_small = y.loc[X_small.index]
else:
    X_small = X
    y_small = y

# Cross-validation function
# ============================
def cv_rmse(model, Xv, yv, n_splits=3):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    scores = cross_val_score(model, Xv, yv, scoring='neg_mean_squared_error', cv=kf, n_jobs=-1)
    rmses = np.sqrt(-scores)
    return rmses

# Keep only columns present in training
X_test = X_test[X.columns]

# 1️⃣ Import required libraries (if not already imported)
from sklearn.tree import DecisionTreeRegressor
from google.colab import files

# 2️⃣ Define the submission function
def make_submission(model, X_test_df, sample_df, filename):
    model.fit(X, y)  # fit on full training data
    preds = model.predict(X_test_df)
    submission = sample_df.copy()
    submission["Purchase"] = preds
    submission.to_csv(filename, index=False)
    print(f"✅ Submission file {filename} created!")
    files.download(filename)

# 3️⃣ Create and evaluate the model
dt_model = DecisionTreeRegressor(max_depth=10, random_state=42)

# CV RMSE
def cv_rmse(model, Xv, yv, n_splits=3):
    from sklearn.model_selection import KFold, cross_val_score
    import numpy as np
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    scores = cross_val_score(model, Xv, yv, scoring='neg_mean_squared_error', cv=kf, n_jobs=-1)
    rmses = np.sqrt(-scores)
    return rmses

print("Decision Tree CV RMSE:", cv_rmse(dt_model, X, y).mean())

# 4️⃣ Generate submission
make_submission(dt_model, X_test, sample_sub, "submission_decisiontree.csv")

# Random Forest
rf_model = RandomForestRegressor(n_estimators=100, max_depth=15, n_jobs=-1, random_state=42)
print("Random Forest CV RMSE:", cv_rmse(rf_model, X, y).mean())
make_submission(rf_model, X_test, sample_sub, "submission_randomforest.csv")

# XGBoost
xgb_model = XGBRegressor(n_estimators=300, learning_rate=0.1, max_depth=8,
                         subsample=0.8, colsample_bytree=0.8, n_jobs=-1, random_state=42)
print("XGBoost CV RMSE:", cv_rmse(xgb_model, X, y).mean())
make_submission(xgb_model, X_test, sample_sub, "submission_xgboost.csv")

# Gradient Boosting
gbr_model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)
print("Gradient Boosting CV RMSE:", cv_rmse(gbr_model, X, y).mean())
make_submission(gbr_model, X_test, sample_sub, "submission_gradientboosting.csv")

# Extra Trees
etr_model = ExtraTreesRegressor(n_estimators=150, max_depth=12, n_jobs=-1, random_state=42)
print("Extra Trees CV RMSE:", cv_rmse(etr_model, X, y).mean())
make_submission(etr_model, X_test, sample_sub, "submission_extratrees.csv")

# HistGradientBoosting
hgb_model = HistGradientBoostingRegressor(max_iter=150, learning_rate=0.08, random_state=42)
print("HistGBR CV RMSE:", cv_rmse(hgb_model, X, y).mean())
make_submission(hgb_model, X_test, sample_sub, "submission_histgbr.csv")

# LightGBM
lgb_model = lgb.LGBMRegressor(n_estimators=300, learning_rate=0.08, max_depth=8,
                              subsample=0.8, colsample_bytree=0.8, n_jobs=-1, random_state=42)
print("LightGBM CV RMSE:", cv_rmse(lgb_model, X, y).mean())
make_submission(lgb_model, X_test, sample_sub, "submission_lightgbm.csv")

# ============================
# 8️⃣ Compare CV results
# ============================
models_rmse = {
    "DecisionTree": cv_rmse(dt_model, X, y).mean(),
    "RandomForest": cv_rmse(rf_model, X, y).mean(),
    "XGBoost": cv_rmse(xgb_model, X, y).mean(),
    "GradientBoosting": cv_rmse(gbr_model, X, y).mean(),
    "ExtraTrees": cv_rmse(etr_model, X, y).mean(),
    "HistGBR": cv_rmse(hgb_model, X, y).mean(),
    "LightGBM": cv_rmse(lgb_model, X, y).mean()
}

# Sort by RMSE
sorted_rmse = dict(sorted(models_rmse.items(), key=lambda item: item[1]))
print("✅ CV RMSE Comparison:")
for model_name, rmse in sorted_rmse.items():
    print(f"{model_name}: {rmse:.2f}")

# ============================
# 9️⃣ Optional: Save models for later
# ============================
import joblib

joblib.dump(dt_model, "decision_tree_model.joblib")
joblib.dump(rf_model, "random_forest_model.joblib")
joblib.dump(xgb_model, "xgboost_model.joblib")
joblib.dump(gbr_model, "gradient_boosting_model.joblib")
joblib.dump(etr_model, "extra_trees_model.joblib")
joblib.dump(hgb_model, "hist_gbr_model.joblib")
joblib.dump(lgb_model, "lightgbm_model.joblib")

print("✅ All models saved as .joblib files")

# ============================
# 10️⃣ Optional: Simple Ensembling (Average of top 3 models)
# ============================
# Load the CSVs (or use the predictions if stored)
top3_files = ["submission_xgboost.csv", "submission_lightgbm.csv", "submission_randomforest.csv"]

dfs = [pd.read_csv(f) for f in top3_files]

# Average predictions
ensemble_preds = sum([df["Purchase"] for df in dfs]) / len(dfs)

# Create ensemble submission
ensemble_submission = sample_sub.copy()
ensemble_submission["Purchase"] = ensemble_preds
ensemble_submission.to_csv("submission_ensemble.csv", index=False)
files.download("submission_ensemble.csv")
print("✅ Ensemble submission file created!")

models_rmse = {
    "DecisionTree": cv_rmse(dt_model, X, y).mean(),
    "RandomForest": cv_rmse(rf_model, X, y).mean(),
    "XGBoost": cv_rmse(xgb_model, X, y).mean(),
    "GradientBoosting": cv_rmse(gbr_model, X, y).mean(),
    "ExtraTrees": cv_rmse(etr_model, X, y).mean(),
    "HistGBR": cv_rmse(hgb_model, X, y).mean(),
    "LightGBM": cv_rmse(lgb_model, X, y).mean()
}

# Sort by RMSE
sorted_rmse = dict(sorted(models_rmse.items(), key=lambda item: item[1]))
print("✅ CV RMSE Comparison:")
for model_name, rmse in sorted_rmse.items():
    print(f"{model_name}: {rmse:.2f}")

import joblib

joblib.dump(dt_model, "decision_tree_model.joblib")
joblib.dump(rf_model, "random_forest_model.joblib")
joblib.dump(xgb_model, "xgboost_model.joblib")
joblib.dump(gbr_model, "gradient_boosting_model.joblib")
joblib.dump(etr_model, "extra_trees_model.joblib")
joblib.dump(hgb_model, "hist_gbr_model.joblib")
joblib.dump(lgb_model, "lightgbm_model.joblib")

print("✅ All models saved as .joblib files")

top3_files = ["submission_xgboost.csv", "submission_lightgbm.csv", "submission_randomforest.csv"]

dfs = [pd.read_csv(f) for f in top3_files]

# Create ensemble submission
ensemble_submission = sample_sub.copy()
ensemble_submission["Purchase"] = ensemble_preds
ensemble_submission.to_csv("submission_ensemble.csv", index=False)
files.download("submission_ensemble.csv")
print("✅ Ensemble submission file created!")



